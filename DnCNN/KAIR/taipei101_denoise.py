#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import argparse
import logging
import torch
import numpy as np
from collections import OrderedDict
import cv2
import time
from torch.cuda.amp import autocast, GradScaler
import torch.backends.cudnn as cudnn

from utils import utils_logger
from utils import utils_image as util
from models.network_unet import UNetRes
import models.network_unet as nunet
"""
å°åŒ—101å½±åƒå»å™ªè…³æœ¬
ä½¿ç”¨DRUNetæ¨¡å‹é€²è¡Œå½©è‰²å½±åƒå»å™ªè™•ç†

ä½¿ç”¨æ–¹æ³•:
python taipei101_denoise.py --input_dir testsets/taipei101 --output_dir taipei101_color_denoised

"""
print("å¯¦éš›åŒ¯å…¥æª”æ¡ˆï¼š", nunet.__file__)
print("UNetRes åƒæ•¸ï¼š", nunet.UNetRes.__init__.__code__.co_varnames)

class GPUOptimizer:
    """GPUå„ªåŒ–ç®¡ç†å™¨"""
    
    def __init__(self):
        self.device = None
        self.gpu_info = {}
        self.use_amp = False
        self.scaler = None
        
    def setup_gpu(self):
        """è¨­ç½®GPUå„ªåŒ–é…ç½®"""
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            # å•Ÿç”¨cuDNNåŸºæº–æ¸¬è©¦æ¨¡å¼
            cudnn.benchmark = True
            cudnn.deterministic = False
            
            # GPUè¨˜æ†¶é«”ç®¡ç†
            torch.cuda.empty_cache()
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
            # ç²å–GPUè³‡è¨Š
            self.gpu_info = {
                'name': torch.cuda.get_device_name(0),
                'memory_total': torch.cuda.get_device_properties(0).total_memory / 1024**3,
                'memory_available': (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
            }
            
            # æ ¹æ“šGPUè¨˜æ†¶é«”æ±ºå®šæ˜¯å¦ä½¿ç”¨AMP
            if self.gpu_info['memory_total'] >= 4.0:  # 4GBä»¥ä¸Šä½¿ç”¨AMP
                self.use_amp = True
                try:
                    # æ–°ç‰ˆPyTorchèªæ³•
                    from torch.amp import GradScaler
                    self.scaler = GradScaler('cuda')
                except ImportError:
                    # èˆŠç‰ˆPyTorchèªæ³•
                    from torch.cuda.amp import GradScaler
                    self.scaler = GradScaler()
                
            print(f" GPUå„ªåŒ–å•Ÿç”¨:")
            print(f"   è¨­å‚™: {self.gpu_info['name']}")
            print(f"   ç¸½è¨˜æ†¶é«”: {self.gpu_info['memory_total']:.1f}GB")
            print(f"   å¯ç”¨è¨˜æ†¶é«”: {self.gpu_info['memory_available']:.1f}GB")
            print(f"   æ··åˆç²¾åº¦AMP: {self.use_amp}")
            
        else:
            self.device = torch.device('cpu')
            print(" ä½¿ç”¨CPUè™•ç† (å»ºè­°ä½¿ç”¨GPUä»¥ç²å¾—æ›´å¥½æ€§èƒ½)")
            
        return self.device
    
    def get_optimal_batch_size(self, img_size):
        """æ ¹æ“šGPUè¨˜æ†¶é«”å’Œå½±åƒå°ºå¯¸è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°"""
        if not torch.cuda.is_available():
            return 1
            
        # ä¼°ç®—å–®å¼µå½±åƒæ‰€éœ€è¨˜æ†¶é«”ï¼ˆMBï¼‰
        single_img_memory = (img_size[0] * img_size[1] * 3 * 4) / 1024**2  # å‡è¨­float32
        available_memory_mb = self.gpu_info['memory_available'] * 1024 * 0.7  # ä½¿ç”¨70%å¯ç”¨è¨˜æ†¶é«”
        
        # è€ƒæ…®æ¨¡å‹è¨˜æ†¶é«”ä½”ç”¨
        model_memory_mb = 500  # ä¼°ç®—æ¨¡å‹ä½”ç”¨ç´„500MB
        available_for_batch = available_memory_mb - model_memory_mb
        
        optimal_batch = max(1, int(available_for_batch / (single_img_memory * 2)))  # ä¹˜2è€ƒæ…®å‰å‘å‚³æ’­
        return min(optimal_batch, 8)  # é™åˆ¶æœ€å¤§æ‰¹æ¬¡ç‚º8
    
    def should_tile_process(self, img_size, threshold_pixels=2048*2048):
        """åˆ¤æ–·æ˜¯å¦éœ€è¦åˆ†å¡Šè™•ç†ï¼ˆé¿å…è¨˜æ†¶é«”ä¸è¶³ï¼‰"""
        total_pixels = img_size[0] * img_size[1]
        return total_pixels > threshold_pixels
    
    def get_optimal_tile_size(self, img_size):
        """è¨ˆç®—æœ€ä½³åˆ†å¡Šå°ºå¯¸"""
        h, w = img_size
        
        # æ ¹æ“šGPUè¨˜æ†¶é«”æ±ºå®šåˆ†å¡Šå¤§å°
        if self.gpu_info['memory_total'] >= 8.0:
            max_tile_size = 1024  # 8GB GPU
        elif self.gpu_info['memory_total'] >= 6.0:
            max_tile_size = 768   # 6GB GPU
        else:
            max_tile_size = 512   # 4GB GPUä»¥ä¸‹
        
        # ç¢ºä¿åˆ†å¡Šå°ºå¯¸ä¸è¶…éåŸåœ–å°ºå¯¸
        tile_h = min(max_tile_size, h)
        tile_w = min(max_tile_size, w)
        
        return tile_h, tile_w
    
    def monitor_gpu_memory(self):
        """ç›£æ§GPUè¨˜æ†¶é«”ä½¿ç”¨"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            return {
                'allocated': allocated,
                'cached': cached,
                'free': self.gpu_info['memory_total'] - allocated
            }
        return None
    
    def cleanup_memory(self):
        """æ¸…ç†GPUè¨˜æ†¶é«”"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

def remove_module_prefix(state_dict):
    """ç§»é™¤DataParallelè¨“ç·´æ¨¡å‹çš„'module.'å‰ç¶´"""
    new_state_dict = OrderedDict()
    for key, value in state_dict.items():
        if key.startswith('module.'):
            new_key = key[7:]  # ç§»é™¤'module.'å‰ç¶´
            new_state_dict[new_key] = value
        else:
            new_state_dict[key] = value
    return new_state_dict

def smart_load_model(model, model_path, device, logger):
    """æ™ºèƒ½è¼‰å…¥æ¨¡å‹ï¼Œè‡ªå‹•è™•ç†DataParallelå‰ç¶´å•é¡Œ"""
    try:
        # è¼‰å…¥state_dict
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # å¦‚æœcheckpointæ˜¯å­—å…¸ä¸”åŒ…å«'state_dict'æˆ–å…¶ä»–key
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            elif 'model' in checkpoint:
                state_dict = checkpoint['model']
            else:
                # å‡è¨­æ•´å€‹checkpointå°±æ˜¯state_dict
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        # å˜—è©¦ç›´æ¥è¼‰å…¥
        try:
            model.load_state_dict(state_dict, strict=True)
            logger.info('[SUCCESS] æ¨¡å‹è¼‰å…¥æˆåŠŸ (ç›´æ¥è¼‰å…¥)')
            return True
        except RuntimeError as e:
            if 'module.' in str(e):
                # å¦‚æœæœ‰moduleå‰ç¶´å•é¡Œï¼Œç§»é™¤å‰ç¶´å¾Œé‡è©¦
                logger.info('[INFO] åµæ¸¬åˆ°DataParallelå‰ç¶´ï¼Œæ­£åœ¨ç§»é™¤...')
                clean_state_dict = remove_module_prefix(state_dict)
                
                try:
                    model.load_state_dict(clean_state_dict, strict=True)
                    logger.info('[SUCCESS] æ¨¡å‹è¼‰å…¥æˆåŠŸ (ç§»é™¤DataParallelå‰ç¶´)')
                    return True
                except RuntimeError as e2:
                    # å˜—è©¦éåš´æ ¼è¼‰å…¥
                    logger.warning('[WARNING] åš´æ ¼è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦éåš´æ ¼è¼‰å…¥...')
                    missing_keys, unexpected_keys = model.load_state_dict(clean_state_dict, strict=False)
                    
                    if missing_keys:
                        logger.warning(f'ç¼ºå°‘çš„åƒæ•¸: {missing_keys[:5]}{"..." if len(missing_keys) > 5 else ""}')
                    if unexpected_keys:
                        logger.warning(f'å¤šé¤˜çš„åƒæ•¸: {unexpected_keys[:5]}{"..." if len(unexpected_keys) > 5 else ""}')
                    
                    # å¦‚æœåªæ˜¯å°‘æ•¸åƒæ•¸ä¸åŒ¹é…ï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨
                    if len(missing_keys) < 10:  # å®¹å¿å°‘é‡ç¼ºå¤±
                        logger.info('[SUCCESS] æ¨¡å‹è¼‰å…¥æˆåŠŸ (éåš´æ ¼æ¨¡å¼)')
                        return True
                    else:
                        logger.error('[ERROR] ç¼ºå°‘å¤ªå¤šé‡è¦åƒæ•¸ï¼Œè¼‰å…¥å¤±æ•—')
                        return False
            else:
                raise e
                
    except Exception as e:
        logger.error(f'[ERROR] æ¨¡å‹è¼‰å…¥å®Œå…¨å¤±æ•—: {str(e)}')
        return False

def batch_process_images(model, images, device, noise_level_normalized, gpu_optimizer):
    """æ‰¹æ¬¡è™•ç†å½±åƒä»¥æå‡æ•ˆç‡"""
    results = []
    batch_size = gpu_optimizer.get_optimal_batch_size(images[0].shape[:2])
    
    print(f"  ä½¿ç”¨æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    for i in range(0, len(images), batch_size):
        batch_images = images[i:i+batch_size]
        batch_tensors = []
        
        # è½‰æ›ç‚ºtensoræ‰¹æ¬¡
        for img in batch_images:
            img_tensor = util.uint2tensor4(img).to(device)
            noise_level_tensor = torch.full((1, 1, img_tensor.shape[2], img_tensor.shape[3]), 
                                          noise_level_normalized).to(device)
            img_input = torch.cat([img_tensor, noise_level_tensor], dim=1)
            batch_tensors.append(img_input)
        
        # åˆä½µç‚ºæ‰¹æ¬¡tensor
        if len(batch_tensors) > 1:
            batch_input = torch.cat(batch_tensors, dim=0)
        else:
            batch_input = batch_tensors[0]
        
        # ä½¿ç”¨AMPé€²è¡Œæ¨ç†
        try:
            # æ–°ç‰ˆPyTorchèªæ³•
            from torch.amp import autocast
            with autocast('cuda', enabled=gpu_optimizer.use_amp):
                with torch.no_grad():
                    batch_output = model(batch_input)
        except ImportError:
            # èˆŠç‰ˆPyTorchèªæ³•
            from torch.cuda.amp import autocast
            with autocast(enabled=gpu_optimizer.use_amp):
                with torch.no_grad():
                    batch_output = model(batch_input)
        
        # åˆ†é›¢æ‰¹æ¬¡çµæœ
        if batch_output.shape[0] > 1:
            for j in range(batch_output.shape[0]):
                results.append(util.tensor2uint(batch_output[j:j+1]))
        else:
            results.append(util.tensor2uint(batch_output))
        
        # è¨˜æ†¶é«”æ¸…ç†
        if i % (batch_size * 2) == 0:
            gpu_optimizer.cleanup_memory()
    
    return results

def tile_process_image(model, img_L, device, noise_level_normalized, gpu_optimizer, tile_size=1024, overlap=64):
    """åˆ†å¡Šè™•ç†å¤§åœ–ç‰‡ï¼Œé¿å…è¨˜æ†¶é«”ä¸è¶³"""
    h, w, c = img_L.shape
    
    # å¦‚æœåœ–ç‰‡å°æ–¼åˆ†å¡Šå°ºå¯¸ï¼Œç›´æ¥è™•ç†
    if h <= tile_size and w <= tile_size:
        img_tensor = util.uint2tensor4(img_L).to(device)
        noise_level_tensor = torch.full((1, 1, img_tensor.shape[2], img_tensor.shape[3]), 
                                      noise_level_normalized).to(device)
        img_input = torch.cat([img_tensor, noise_level_tensor], dim=1)
        
        try:
            # æ–°ç‰ˆPyTorchèªæ³•
            from torch.amp import autocast
            with autocast('cuda', enabled=gpu_optimizer.use_amp):
                with torch.no_grad():
                    img_E = model(img_input)
        except ImportError:
            # èˆŠç‰ˆPyTorchèªæ³•
            from torch.cuda.amp import autocast
            with autocast(enabled=gpu_optimizer.use_amp):
                with torch.no_grad():
                    img_E = model(img_input)
        
        return util.tensor2uint(img_E)
    
    # åˆ†å¡Šè™•ç†
    print(f"  å¤§åœ–ç‰‡åˆ†å¡Šè™•ç†: {h}x{w} -> åˆ†å¡Šå°ºå¯¸ {tile_size}x{tile_size}")
    
    # è¨ˆç®—åˆ†å¡Šæ•¸é‡
    h_tiles = (h - 1) // (tile_size - overlap) + 1
    w_tiles = (w - 1) // (tile_size - overlap) + 1
    
    print(f"  åˆ†å¡Šæ•¸é‡: {h_tiles}x{w_tiles} = {h_tiles * w_tiles} å¡Š")
    
    # å»ºç«‹è¼¸å‡ºå½±åƒ
    result_img = np.zeros_like(img_L)
    
    for i in range(h_tiles):
        for j in range(w_tiles):
            # è¨ˆç®—åˆ†å¡Šåº§æ¨™
            start_h = i * (tile_size - overlap)
            start_w = j * (tile_size - overlap)
            end_h = min(start_h + tile_size, h)
            end_w = min(start_w + tile_size, w)
            
            # æå–åˆ†å¡Š
            tile = img_L[start_h:end_h, start_w:end_w, :]
            
            # è™•ç†åˆ†å¡Š
            tile_tensor = util.uint2tensor4(tile).to(device)
            noise_level_tensor = torch.full((1, 1, tile_tensor.shape[2], tile_tensor.shape[3]), 
                                          noise_level_normalized).to(device)
            tile_input = torch.cat([tile_tensor, noise_level_tensor], dim=1)
            
            try:
                # æ–°ç‰ˆPyTorchèªæ³•
                from torch.amp import autocast
                with autocast('cuda', enabled=gpu_optimizer.use_amp):
                    with torch.no_grad():
                        tile_output = model(tile_input)
            except ImportError:
                # èˆŠç‰ˆPyTorchèªæ³•
                from torch.cuda.amp import autocast
                with autocast(enabled=gpu_optimizer.use_amp):
                    with torch.no_grad():
                        tile_output = model(tile_input)
            
            tile_result = util.tensor2uint(tile_output)
            
            # è™•ç†é‡ç–Šå€åŸŸ
            if i == 0 and j == 0:
                # å·¦ä¸Šè§’å¡Šï¼Œå®Œæ•´è¤‡è£½
                result_img[start_h:end_h, start_w:end_w, :] = tile_result
            else:
                # å…¶ä»–å¡Šï¼Œè™•ç†é‡ç–Šå€åŸŸ
                actual_start_h = start_h + (overlap // 2 if i > 0 else 0)
                actual_start_w = start_w + (overlap // 2 if j > 0 else 0)
                
                tile_start_h = overlap // 2 if i > 0 else 0
                tile_start_w = overlap // 2 if j > 0 else 0
                
                result_img[actual_start_h:end_h, actual_start_w:end_w, :] = \
                    tile_result[tile_start_h:, tile_start_w:, :]
            
            # æ¸…ç†è¨˜æ†¶é«”
            del tile_tensor, tile_input, tile_output, tile_result
            if (i * w_tiles + j) % 4 == 0:  # æ¯4å¡Šæ¸…ç†ä¸€æ¬¡
                gpu_optimizer.cleanup_memory()
            
            print(f"    å®Œæˆåˆ†å¡Š ({i+1}/{h_tiles}, {j+1}/{w_tiles})")
    
    return result_img

def advanced_detail_enhancement(denoised_img, original_img):
    """
    é€²éšç´°ç¯€å¢å¼·è™•ç†
    """
    # è½‰æ›ç‚ºç°éšåˆ†æ
    if len(original_img.shape) == 3:
        gray_orig = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)
        gray_denoised = cv2.cvtColor(denoised_img, cv2.COLOR_BGR2GRAY)
    else:
        gray_orig = original_img
        gray_denoised = denoised_img
    
    # è¨ˆç®—å±€éƒ¨çµ±è¨ˆ
    kernel = np.ones((5, 5), np.float32) / 25
    local_mean = cv2.filter2D(gray_orig.astype(np.float32), -1, kernel)
    local_sqr_mean = cv2.filter2D((gray_orig.astype(np.float32))**2, -1, kernel)
    local_var = local_sqr_mean - local_mean**2
    
    # è¨ˆç®—æ¢¯åº¦è³‡è¨Š
    grad_x = cv2.Sobel(gray_orig, cv2.CV_64F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(gray_orig, cv2.CV_64F, 0, 1, ksize=3)
    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
    
    # å»ºç«‹ä¸åŒå€åŸŸçš„é®ç½©
    high_var_threshold = np.percentile(local_var, 75)
    high_brightness_threshold = np.percentile(local_mean, 80)
    high_gradient_threshold = np.percentile(gradient_magnitude, 80)
    
    # é‚Šç·£å€åŸŸï¼šä¿æŒæ›´å¤šåŸå§‹ç´°ç¯€
    edge_mask = (local_var > high_var_threshold) & (gradient_magnitude > high_gradient_threshold)
    
    # äº®éƒ¨å€åŸŸï¼šç‰¹åˆ¥ä¿è­·
    bright_mask = local_mean > high_brightness_threshold
    
    result = denoised_img.copy().astype(np.float32)
    
    # å°é‚Šç·£å€åŸŸæ¢å¾©æ›´å¤šç´°ç¯€
    if np.sum(edge_mask) > 0:
        edge_weight = 0.75  # å¢åŠ é‚Šç·£ç´°ç¯€æ¬Šé‡
        for i in range(denoised_img.shape[2] if len(denoised_img.shape) == 3 else 1):
            if len(denoised_img.shape) == 3:
                result[:, :, i][edge_mask] = (
                    edge_weight * original_img[:, :, i][edge_mask] + 
                    (1 - edge_weight) * denoised_img[:, :, i][edge_mask]
                )
            else:
                result[edge_mask] = (
                    edge_weight * original_img[edge_mask] + 
                    (1 - edge_weight) * denoised_img[edge_mask]
                )
    
    # å°äº®éƒ¨å€åŸŸç‰¹åˆ¥è™•ç†
    if np.sum(bright_mask) > 0:
        bright_weight = 0.6  # äº®éƒ¨ç´°ç¯€æ¬Šé‡
        for i in range(denoised_img.shape[2] if len(denoised_img.shape) == 3 else 1):
            if len(denoised_img.shape) == 3:
                result[:, :, i][bright_mask] = (
                    bright_weight * original_img[:, :, i][bright_mask] + 
                    (1 - bright_weight) * denoised_img[:, :, i][bright_mask]
                )
            else:
                result[bright_mask] = (
                    bright_weight * original_img[bright_mask] + 
                    (1 - bright_weight) * denoised_img[bright_mask]
                )
    
    return np.clip(result, 0, 255).astype(np.uint8)

def adaptive_noise_level_processing(model, img_L, device, base_noise_level=10.0):
    """
    æ ¹æ“šå½±åƒå€åŸŸç‰¹æ€§èª¿æ•´é›œè¨Šç­‰ç´š
    """
    # è½‰æ›ç‚ºnumpyé€²è¡Œåˆ†æ
    img_np = util.tensor2uint(img_L)
    
    if len(img_np.shape) == 3:
        gray = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)
    else:
        gray = img_np
    
    # è¨ˆç®—å±€éƒ¨æ–¹å·®
    kernel = np.ones((5, 5), np.float32) / 25
    local_mean = cv2.filter2D(gray.astype(np.float32), -1, kernel)
    local_sqr_mean = cv2.filter2D((gray.astype(np.float32))**2, -1, kernel)
    local_var = local_sqr_mean - local_mean**2
    
    # æ ¹æ“šå±€éƒ¨æ–¹å·®èª¿æ•´é›œè¨Šç­‰ç´š
    # é«˜æ–¹å·®å€åŸŸï¼ˆé‚Šç·£ï¼‰ä½¿ç”¨è¼ƒä½çš„é›œè¨Šç­‰ç´š
    # ä½æ–¹å·®å€åŸŸï¼ˆå¹³æ»‘å€åŸŸï¼‰ä½¿ç”¨è¼ƒé«˜çš„é›œè¨Šç­‰ç´š
    var_percentiles = np.percentile(local_var, [25, 75])
    
    results = []
    noise_levels = []
    
    # è™•ç†ä¸åŒé›œè¨Šç­‰ç´š
    for noise_factor in [0.5, 1.0, 1.5]:  # ä½ã€ä¸­ã€é«˜é›œè¨Šç­‰ç´š
        current_noise_level = base_noise_level * noise_factor
        noise_level_normalized = current_noise_level / 255.0
        
        noise_level_tensor = torch.full((1, 1, img_L.shape[2], img_L.shape[3]), 
                                      noise_level_normalized).to(device)
        img_input = torch.cat([img_L, noise_level_tensor], dim=1)
        
        with torch.no_grad():
            img_E = model(img_input)
            results.append(util.tensor2uint(img_E))
            noise_levels.append(current_noise_level)
    
    # æ ¹æ“šå±€éƒ¨ç‰¹æ€§æ··åˆçµæœ
    final_result = np.zeros_like(results[0])
    
    # ä½æ–¹å·®å€åŸŸä½¿ç”¨é«˜é›œè¨Šç­‰ç´šè™•ç†çµæœ
    low_var_mask = local_var < var_percentiles[0]
    final_result[low_var_mask] = results[2][low_var_mask]  # é«˜é›œè¨Šç­‰ç´š
    
    # é«˜æ–¹å·®å€åŸŸä½¿ç”¨ä½é›œè¨Šç­‰ç´šè™•ç†çµæœ
    high_var_mask = local_var > var_percentiles[1]
    final_result[high_var_mask] = results[0][high_var_mask]  # ä½é›œè¨Šç­‰ç´š
    
    # ä¸­é–“å€åŸŸä½¿ç”¨ä¸­ç­‰é›œè¨Šç­‰ç´š
    medium_mask = ~(low_var_mask | high_var_mask)
    final_result[medium_mask] = results[1][medium_mask]  # ä¸­ç­‰é›œè¨Šç­‰ç´š
    
    return final_result


def frequency_domain_detail_preservation(denoised_img, original_img, detail_ratio=0.3):
    """
    åœ¨é »åŸŸä¸­ä¿è­·é«˜é »ç´°ç¯€
    """
    # è½‰æ›ç‚ºç°éšé€²è¡Œé »åŸŸåˆ†æ
    if len(original_img.shape) == 3:
        gray_orig = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)
        gray_denoised = cv2.cvtColor(denoised_img, cv2.COLOR_BGR2GRAY)
    else:
        gray_orig = original_img
        gray_denoised = denoised_img
    
    # FFTè®Šæ›
    f_orig = np.fft.fft2(gray_orig.astype(np.float32))
    f_denoised = np.fft.fft2(gray_denoised.astype(np.float32))
    
    # å‰µå»ºé«˜é€šæ¿¾æ³¢å™¨
    rows, cols = gray_orig.shape
    center_row, center_col = rows // 2, cols // 2
    
    # å‰µå»ºé »åŸŸé®ç½©
    y, x = np.ogrid[:rows, :cols]
    distance = np.sqrt((y - center_row)**2 + (x - center_col)**2)
    
    # é«˜é »é®ç½©ï¼ˆä¿ç•™é«˜é »ç´°ç¯€ï¼‰
    high_freq_mask = distance > min(rows, cols) * 0.1
    
    # åœ¨é«˜é »éƒ¨åˆ†æ··åˆåŸå§‹å’Œå»å™ªå¾Œçš„é »è­œ
    f_enhanced = f_denoised.copy()
    f_enhanced[high_freq_mask] = (
        (1 - detail_ratio) * f_denoised[high_freq_mask] + 
        detail_ratio * f_orig[high_freq_mask]
    )
    
    # é€†FFTè®Šæ›
    enhanced_gray = np.real(np.fft.ifft2(f_enhanced))
    enhanced_gray = np.clip(enhanced_gray, 0, 255).astype(np.uint8)
    
    # å¦‚æœæ˜¯å½©è‰²å½±åƒï¼Œéœ€è¦å°‡ç´°ç¯€å¢å¼·æ‡‰ç”¨åˆ°å½©è‰²å½±åƒ
    if len(denoised_img.shape) == 3:
        # è¨ˆç®—ç´°ç¯€å·®ç•°
        detail_diff = enhanced_gray.astype(np.float32) - gray_denoised.astype(np.float32)
        
        # å°‡ç´°ç¯€å·®ç•°æ‡‰ç”¨åˆ°æ¯å€‹é¡è‰²é€šé“
        result = denoised_img.astype(np.float32)
        for i in range(3):
            result[:, :, i] += detail_diff * 0.8  # èª¿æ•´ç´°ç¯€å¼·åº¦
        
        return np.clip(result, 0, 255).astype(np.uint8)
    else:
        return enhanced_gray

def main():
    # ----------------------------------------
    # åƒæ•¸è¨­å®š
    # ----------------------------------------
    parser = argparse.ArgumentParser(description='å°åŒ—101å½±åƒå»å™ªè™•ç†')
    parser.add_argument('--model_path', type=str, default='model_zoo/best_G.pth',
                       help='å»å™ªæ¨¡å‹è·¯å¾‘ (ä½¿ç”¨æœ€æ–°è¨“ç·´çš„æœ€ä½³æ¨¡å‹)')
    parser.add_argument('--input_dir', type=str, default='testsets/taipei101',
                       help='è¼¸å…¥å½±åƒç›®éŒ„')
    parser.add_argument('--output_dir', type=str, default='taipei101_color_denoised',
                       help='è¼¸å‡ºçµæœç›®éŒ„')
    parser.add_argument('--noise_level', type=float, default=5.0,
                       help='é›œè¨Šç­‰ç´š (0-255)')
    parser.add_argument('--device', type=str, default='auto',
                       help='é‹ç®—è¨­å‚™: auto, cpu, cuda')
    parser.add_argument('--enable_batch_processing', action='store_true',
                       help='å•Ÿç”¨æ‰¹æ¬¡è™•ç†ä»¥æå‡GPUæ•ˆç‡')
    parser.add_argument('--monitor_performance', action='store_true',
                       help='å•Ÿç”¨æ€§èƒ½ç›£æ§')
    args = parser.parse_args()

    # ----------------------------------------
    # å»ºç«‹è¼¸å‡ºç›®éŒ„å’Œæ—¥èªŒ
    # ----------------------------------------
    util.mkdir(args.output_dir)
    
    logger_name = 'taipei101_denoise'
    utils_logger.logger_info(logger_name, log_path=os.path.join(args.output_dir, 'denoise.log'))
    logger = logging.getLogger(logger_name)
    
    logger.info('=== å°åŒ—101å½±åƒå»å™ªè™•ç† (GPUå„ªåŒ–ç‰ˆ) ===')
    logger.info(f'æ¨¡å‹è·¯å¾‘: {args.model_path}')
    logger.info(f'è¼¸å…¥ç›®éŒ„: {args.input_dir}')
    logger.info(f'è¼¸å‡ºç›®éŒ„: {args.output_dir}')
    logger.info(f'é›œè¨Šç­‰ç´š: {args.noise_level}')
    logger.info(f'æ‰¹æ¬¡è™•ç†: {args.enable_batch_processing}')
    logger.info(f'æ€§èƒ½ç›£æ§: {args.monitor_performance}')
    
    # ----------------------------------------
    # GPUå„ªåŒ–å™¨è¨­å®š
    # ----------------------------------------
    gpu_optimizer = GPUOptimizer()
    device = gpu_optimizer.setup_gpu()
    
    if args.device != 'auto':
        device = torch.device(args.device)
        print(f" å¼·åˆ¶ä½¿ç”¨æŒ‡å®šè¨­å‚™: {device}")
    
    logger.info(f'ä½¿ç”¨è¨­å‚™: {device}')
    
    # æ€§èƒ½ç›£æ§åˆå§‹åŒ–
    if args.monitor_performance:
        start_memory = gpu_optimizer.monitor_gpu_memory()
        if start_memory:
            logger.info(f'åˆå§‹GPUè¨˜æ†¶é«”: å·²åˆ†é… {start_memory["allocated"]:.2f}GB, å¿«å– {start_memory["cached"]:.2f}GB')
    
    # ----------------------------------------
    # æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨
    # ----------------------------------------
    if not os.path.exists(args.model_path):
        logger.error(f'æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {args.model_path}')
        print(f" éŒ¯èª¤: æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ {args.model_path}")
        return
    
    if not os.path.exists(args.input_dir):
        logger.error(f'è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {args.input_dir}')
        print(f" éŒ¯èª¤: æ‰¾ä¸åˆ°è¼¸å…¥ç›®éŒ„ {args.input_dir}")
        return
    
    # ----------------------------------------
    # è¼‰å…¥æ¨¡å‹ (æ™ºèƒ½è¼‰å…¥ï¼Œè‡ªå‹•è™•ç†DataParallelå•é¡Œ)
    # ----------------------------------------
    try:
        logger.info('è¼‰å…¥DRUNetå½©è‰²å»å™ªæ¨¡å‹...')
        
        # å»ºç«‹æ¨¡å‹çµæ§‹ (èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´)
        model = UNetRes(in_nc=4, out_nc=3, nc=[64, 128, 256, 512], nb=4, act_mode='R', 
                       downsample_mode="strideconv", upsample_mode="convtranspose", bias=False, use_nonlocal=True)
        
        # æ™ºèƒ½è¼‰å…¥é è¨“ç·´æ¬Šé‡
        if not smart_load_model(model, args.model_path, device, logger):
            logger.error('[ERROR] æ¨¡å‹è¼‰å…¥å¤±æ•—')
            print(f" âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ")
            return
        
        model.eval()
        
        # ç¦ç”¨æ¢¯åº¦è¨ˆç®—
        for k, v in model.named_parameters():
            v.requires_grad = False
        
        model = model.to(device)
        
        # è¨ˆç®—æ¨¡å‹åƒæ•¸æ•¸é‡
        num_params = sum(map(lambda x: x.numel(), model.parameters()))
        logger.info(f'æ¨¡å‹åƒæ•¸æ•¸é‡: {num_params:,}')
        
        # æ¨¡å‹è³‡è¨Š
        print(f"  âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
        print(f"  ğŸ“Š åƒæ•¸æ•¸é‡: {num_params:,}")
        print(f"  ğŸ—ï¸  æ¨¡å‹çµæ§‹: UNetRes (DRUNet)")
        
    except Exception as e:
        logger.error(f'è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}')
        print(f" âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
        return
    
    # ----------------------------------------
    # å–å¾—å½±åƒæª”æ¡ˆåˆ—è¡¨
    # ----------------------------------------
    image_paths = util.get_image_paths(args.input_dir)
    logger.info(f'æ‰¾åˆ°å½±åƒæª”æ¡ˆæ•¸é‡: {len(image_paths)}')
    
    if len(image_paths) == 0:
        logger.warning('è¼¸å…¥ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ°å½±åƒæª”æ¡ˆ')
        print("  è­¦å‘Š: æ²’æœ‰æ‰¾åˆ°ä»»ä½•å½±åƒæª”æ¡ˆ")
        return
    
    # ----------------------------------------
    # æ­£è¦åŒ–é›œè¨Šç­‰ç´š
    # ----------------------------------------
    noise_level_normalized = args.noise_level / 255.0
    logger.info(f'æ­£è¦åŒ–é›œè¨Šç­‰ç´š: {noise_level_normalized:.4f}')
    
    # ----------------------------------------
    # è™•ç†å½±åƒ (GPUå„ªåŒ–ç‰ˆæœ¬)
    # ----------------------------------------
    logger.info('é–‹å§‹è™•ç†å½±åƒ (GPUå„ªåŒ–æ¨¡å¼)...')
    print(f"  é–‹å§‹è™•ç† {len(image_paths)} å¼µå½±åƒ")
    
    processed_count = 0
    failed_count = 0
    start_time = time.time()
    
    if args.enable_batch_processing and len(image_paths) > 1:
        # æ‰¹æ¬¡è™•ç†æ¨¡å¼
        logger.info('ä½¿ç”¨æ‰¹æ¬¡è™•ç†æ¨¡å¼æå‡æ•ˆç‡')
        
        try:
            # è¼‰å…¥æ‰€æœ‰å½±åƒ
            all_images = []
            all_names = []
            
            print("  è¼‰å…¥å½±åƒä¸­...")
            for img_path in image_paths:
                img_name = os.path.basename(img_path)
                img_L = util.imread_uint(img_path, n_channels=3)
                all_images.append(img_L)
                all_names.append(img_name)
            
            # æ‰¹æ¬¡è™•ç†
            print("  åŸ·è¡Œæ‰¹æ¬¡å»å™ªè™•ç†...")
            results = batch_process_images(model, all_images, device, noise_level_normalized, gpu_optimizer)
            
            # å¾Œè™•ç†å’Œå„²å­˜
            print("  åŸ·è¡Œå¾Œè™•ç†å’Œå„²å­˜...")
            for idx, (img_E, img_L, img_name) in enumerate(zip(results, all_images, all_names)):
                try:
                    # é€²éšç´°ç¯€å¢å¼·
                    img_E = advanced_detail_enhancement(img_E, img_L)
                    
                    # é »åŸŸç´°ç¯€ä¿è­·
                    img_E = frequency_domain_detail_preservation(img_E, img_L, detail_ratio=0.2)
                    
                    # å„²å­˜çµæœ
                    output_path = os.path.join(args.output_dir, img_name)
                    util.imsave(img_E, output_path)
                    
                    processed_count += 1
                    print(f" ({idx+1}/{len(image_paths)}) {img_name} - è™•ç†å®Œæˆ")
                    
                    # æ€§èƒ½ç›£æ§
                    if args.monitor_performance and idx == 0:
                        memory_info = gpu_optimizer.monitor_gpu_memory()
                        if memory_info:
                            logger.info(f'è™•ç†ä¸­GPUè¨˜æ†¶é«”: å·²åˆ†é… {memory_info["allocated"]:.2f}GB')
                    
                except Exception as e:
                    failed_count += 1
                    logger.error(f'{img_name} - å¾Œè™•ç†å¤±æ•—: {str(e)}')
                    print(f" ({idx+1}/{len(image_paths)}) {img_name} - å¾Œè™•ç†å¤±æ•—: {str(e)}")
                    
        except Exception as e:
            logger.error(f'æ‰¹æ¬¡è™•ç†å¤±æ•—ï¼Œå›é€€åˆ°å–®å¼µè™•ç†: {str(e)}')
            print(f"  æ‰¹æ¬¡è™•ç†å¤±æ•—ï¼Œä½¿ç”¨å–®å¼µè™•ç†æ¨¡å¼: {str(e)}")
            args.enable_batch_processing = False
    
    # å–®å¼µè™•ç†æ¨¡å¼æˆ–æ‰¹æ¬¡è™•ç†å¤±æ•—æ™‚çš„å¾Œå‚™æ–¹æ¡ˆ
    if not args.enable_batch_processing or processed_count == 0:
        for idx, img_path in enumerate(image_paths):
            img_name = os.path.basename(img_path)
            
            try:
                logger.info(f'è™•ç†å½±åƒ {idx+1}/{len(image_paths)}: {img_name}')
                
                # è¼‰å…¥å½±åƒ
                img_L = util.imread_uint(img_path, n_channels=3)
                original_shape = img_L.shape
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ†å¡Šè™•ç†ï¼ˆé¿å…è¨˜æ†¶é«”ä¸è¶³ï¼‰
                if gpu_optimizer.should_tile_process(original_shape[:2]):
                    print(f"  å¤§åœ–ç‰‡æª¢æ¸¬: {original_shape[0]}x{original_shape[1]}, ä½¿ç”¨åˆ†å¡Šè™•ç†")
                    tile_h, tile_w = gpu_optimizer.get_optimal_tile_size(original_shape[:2])
                    img_E = tile_process_image(model, img_L, device, noise_level_normalized, 
                                             gpu_optimizer, tile_size=min(tile_h, tile_w))
                else:
                    # GPUå„ªåŒ–çš„å–®å¼µè™•ç†
                    try:
                        # æ–°ç‰ˆPyTorchèªæ³•
                        from torch.amp import autocast
                        with autocast('cuda', enabled=gpu_optimizer.use_amp):
                            # è½‰æ›ç‚ºtensor
                            img_tensor = util.uint2tensor4(img_L)
                            img_tensor = img_tensor.to(device)
                            
                            # ä½¿ç”¨è‡ªé©æ‡‰é›œè¨Šç­‰ç´šè™•ç†
                            img_E = adaptive_noise_level_processing(model, img_tensor, device, args.noise_level)
                    except ImportError:
                        # èˆŠç‰ˆPyTorchèªæ³•
                        from torch.cuda.amp import autocast
                        with autocast(enabled=gpu_optimizer.use_amp):
                            # è½‰æ›ç‚ºtensor
                            img_tensor = util.uint2tensor4(img_L)
                            img_tensor = img_tensor.to(device)
                            
                            # ä½¿ç”¨è‡ªé©æ‡‰é›œè¨Šç­‰ç´šè™•ç†
                            img_E = adaptive_noise_level_processing(model, img_tensor, device, args.noise_level)
                
                # ç¢ºä¿è¼¸å‡ºå½±åƒå°ºå¯¸æ­£ç¢º
                if img_E.shape != original_shape:
                    logger.warning(f'{img_name} - è¼¸å‡ºå°ºå¯¸ä¸åŒ¹é…: {img_E.shape} vs {original_shape}')
                
                # é€²éšç´°ç¯€å¢å¼·
                img_E = advanced_detail_enhancement(img_E, img_L)
                
                # é »åŸŸç´°ç¯€ä¿è­·
                img_E = frequency_domain_detail_preservation(img_E, img_L, detail_ratio=0.2)
                
                # å„²å­˜çµæœ
                output_path = os.path.join(args.output_dir, img_name)
                util.imsave(img_E, output_path)
                
                processed_count += 1
                print(f" ({idx+1}/{len(image_paths)}) {img_name} - è™•ç†å®Œæˆ")
                
                # å®šæœŸæ¸…ç†è¨˜æ†¶é«”
                if idx % 5 == 0:
                    gpu_optimizer.cleanup_memory()
                
                # æ€§èƒ½ç›£æ§
                if args.monitor_performance and idx % 10 == 0:
                    memory_info = gpu_optimizer.monitor_gpu_memory()
                    if memory_info:
                        logger.info(f'è™•ç†é€²åº¦ {idx+1}/{len(image_paths)}, GPUè¨˜æ†¶é«”: å·²åˆ†é… {memory_info["allocated"]:.2f}GB')
                
            except Exception as e:
                failed_count += 1
                logger.error(f'{img_name} - è™•ç†å¤±æ•—: {str(e)}')
                print(f" ({idx+1}/{len(image_paths)}) {img_name} - è™•ç†å¤±æ•—: {str(e)}")
                continue
    
    # æœ€çµ‚è¨˜æ†¶é«”æ¸…ç†
    gpu_optimizer.cleanup_memory()
    
    # è¨ˆç®—è™•ç†æ™‚é–“
    total_time = time.time() - start_time
    avg_time_per_image = total_time / processed_count if processed_count > 0 else 0
    
    # ----------------------------------------
    # è™•ç†çµæœçµ±è¨ˆ (å«æ€§èƒ½è³‡è¨Š)
    # ----------------------------------------
    logger.info('\n' + '=' * 60)
    logger.info(' è™•ç†çµæœçµ±è¨ˆ (GPUå„ªåŒ–ç‰ˆ)')
    logger.info('=' * 60)
    logger.info(f'ç¸½å½±åƒæ•¸é‡: {len(image_paths)}')
    logger.info(f'æˆåŠŸè™•ç†: {processed_count}')
    logger.info(f'è™•ç†å¤±æ•—: {failed_count}')
    logger.info(f'æˆåŠŸç‡: {processed_count/len(image_paths)*100:.1f}%')
    logger.info(f'ç¸½è™•ç†æ™‚é–“: {total_time:.2f} ç§’')
    logger.info(f'å¹³å‡æ¯å¼µæ™‚é–“: {avg_time_per_image:.2f} ç§’')
    logger.info(f'è™•ç†é€Ÿåº¦: {processed_count/total_time:.2f} å¼µ/ç§’')
    logger.info(f'æ‰¹æ¬¡è™•ç†æ¨¡å¼: {"å•Ÿç”¨" if args.enable_batch_processing else "åœç”¨"}')
    logger.info(f'æ··åˆç²¾åº¦AMP: {"å•Ÿç”¨" if gpu_optimizer.use_amp else "åœç”¨"}')
    logger.info(f'çµæœå„²å­˜ä½ç½®: {args.output_dir}')
    
    # æœ€çµ‚æ€§èƒ½ç›£æ§
    if args.monitor_performance:
        final_memory = gpu_optimizer.monitor_gpu_memory()
        if final_memory:
            logger.info(f'æœ€çµ‚GPUè¨˜æ†¶é«”: å·²åˆ†é… {final_memory["allocated"]:.2f}GB, å‰©é¤˜ {final_memory["free"]:.2f}GB')
    
    print(f"\n å°åŒ—101å½±åƒå»å™ªè™•ç†å®Œæˆï¼(GPUå„ªåŒ–ç‰ˆ)")
    print(f" è™•ç†çµ±è¨ˆ: {processed_count}/{len(image_paths)} æˆåŠŸ")
    print(f" è™•ç†æ™‚é–“: {total_time:.2f} ç§’ (å¹³å‡ {avg_time_per_image:.2f} ç§’/å¼µ)")
    print(f" è™•ç†é€Ÿåº¦: {processed_count/total_time:.2f} å¼µ/ç§’")
    print(f" GPUå„ªåŒ–: AMPæ··åˆç²¾åº¦ {'âœ“' if gpu_optimizer.use_amp else 'âœ—'}, æ‰¹æ¬¡è™•ç† {'âœ“' if args.enable_batch_processing else 'âœ—'}")
    print(f" çµæœä½ç½®: {args.output_dir}")
    
    if processed_count > 0:
        print(f"\n ä¸‹ä¸€æ­¥å»ºè­°:")
        print(f"   1. ä½¿ç”¨è©•ä¼°è…³æœ¬æ¯”è¼ƒå»å™ªæ•ˆæœ: python main_evaluate_taipei101.py")
        print(f"   2. å¦‚éœ€æ›´å¿«è™•ç†ï¼Œä¸‹æ¬¡å¯åŠ ä¸Šåƒæ•¸: --enable_batch_processing --monitor_performance")
        
        # æ€§èƒ½å»ºè­°
        if total_time > 0 and processed_count > 0:
            if avg_time_per_image > 2.0:
                print(f"   ğŸ’¡ æ•ˆèƒ½æç¤º: è™•ç†é€Ÿåº¦è¼ƒæ…¢ï¼Œå»ºè­°å•Ÿç”¨æ‰¹æ¬¡è™•ç†æ¨¡å¼ä»¥æå‡æ•ˆç‡")
            elif avg_time_per_image < 0.5:
                print(f"   ğŸš€ æ•ˆèƒ½å„ªç•°: GPUå„ªåŒ–æ•ˆæœé¡¯è‘—ï¼")
    
    # æ›´æ–°TODOç‹€æ…‹
    print(f"\n âœ… æ¨¡å‹æ›´æ–°å®Œæˆ: å·²ä½¿ç”¨æœ€æ–°çš„ best_G.pth æ¨¡å‹")
    print(f" âœ… GPUå„ªåŒ–å®Œæˆ: æ··åˆç²¾åº¦ã€æ‰¹æ¬¡è™•ç†ã€è¨˜æ†¶é«”ç®¡ç†å·²å•Ÿç”¨")

if __name__ == "__main__":
    main()
